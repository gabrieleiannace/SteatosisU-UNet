{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68549992",
   "metadata": {},
   "source": [
    "# Steatosis Segmentation using U-Net\n",
    "Training a U-Net model to segment liver steatosis using the provided Training and Validation datasets.\n",
    "This notebook implements the training pipeline including data loading, model definition, training loop, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dde62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Configuration\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 20\n",
    "INPUT_SIZE = (256, 256)\n",
    "CHECKPOINT_DIR = 'checkpoints_steatosis'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Data paths (Modify these if your folder structure is different)\n",
    "TRAIN_DIR = 'train'\n",
    "VAL_DIR = 'val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f901b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteatosisDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with 'image' and 'manual' subdirs.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.image_dir = os.path.join(root_dir, 'image')\n",
    "        # MODIFICA: Usiamo la cartella corretta per il training\n",
    "        self.mask_dir = os.path.join(root_dir, 'manual_py')\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load file lists\n",
    "        # We filter for common image extensions\n",
    "        valid_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.tif')\n",
    "        self.images = sorted([f for f in os.listdir(self.image_dir) if f.lower().endswith(valid_extensions)])\n",
    "        self.masks = sorted([f for f in os.listdir(self.mask_dir) if f.lower().endswith(valid_extensions)])\n",
    "        \n",
    "        # Basic check\n",
    "        if len(self.images) != len(self.masks):\n",
    "            print(f\"Warning: Number of images ({len(self.images)}) and masks ({len(self.masks)}) in {root_dir} do not match!\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        mask_name = self.masks[idx] \n",
    "        \n",
    "        # Construct full paths\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
    "        \n",
    "        # Load image and mask\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\") # Grayscale for mask\n",
    "        \n",
    "        # Apply transforms\n",
    "        \n",
    "        # Resize image (Bilineare va bene per le foto)\n",
    "        resize_img = transforms.Resize(INPUT_SIZE, interpolation=transforms.InterpolationMode.BILINEAR)\n",
    "        image = resize_img(image)\n",
    "        \n",
    "        # Resize mask (NEAREST è fondamentale per le maschere per mantenere 0 e 1 puri)\n",
    "        resize_mask = transforms.Resize(INPUT_SIZE, interpolation=transforms.InterpolationMode.NEAREST)\n",
    "        mask = resize_mask(mask)\n",
    "        \n",
    "        to_tensor = transforms.ToTensor()\n",
    "        \n",
    "        image = to_tensor(image)\n",
    "        mask = to_tensor(mask)\n",
    "        \n",
    "        # Con manual_py i valori sono spesso molto bassi (0 e 1 su scala 255 vengono caricati quasi neri).\n",
    "        # ToTensor normalizza in [0, 1]. Se il pixel era 1 (classe steatosi), diventa 1/255.\n",
    "        # Se invece manual_py è salvata come 0 e 255, diventa 0 e 1.\n",
    "        # Per sicurezza assoluta, binarizziamo qualunque cosa > 0.\n",
    "        mask = (mask > 0).float()\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c069690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset instances\n",
    "train_dataset = SteatosisDataset(TRAIN_DIR)\n",
    "val_dataset = SteatosisDataset(VAL_DIR)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Training set: {len(train_dataset)} images\")\n",
    "print(f\"Validation set: {len(val_dataset)} images\")\n",
    "\n",
    "# Sanity check: Visualize one sample\n",
    "temp_img, temp_mask = train_dataset[0]\n",
    "print(f\"Image tensor shape: {temp_img.shape}\")\n",
    "print(f\"Mask tensor shape: {temp_mask.shape}\")\n",
    "print(f\"Mask unique values: {torch.unique(temp_mask)}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(temp_img.permute(1, 2, 0))\n",
    "plt.title(\"Sample Image\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(temp_mask.squeeze(), cmap='gray')\n",
    "plt.title(\"Sample Mask\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdd012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"Applies two consecutive conv-batchnorm-relu layers\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels=3,\n",
    "                 out_channels=1,\n",
    "                 init_filters=64,\n",
    "                 depth=4,\n",
    "                 bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.down_layers = nn.ModuleList()\n",
    "        self.up_layers = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # Encoder\n",
    "        filters = init_filters\n",
    "        for d in range(depth):\n",
    "            conv = DoubleConv(in_channels, filters)\n",
    "            self.down_layers.append(conv)\n",
    "            in_channels = filters\n",
    "            filters *= 2\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv(in_channels, filters)\n",
    "\n",
    "        # Decoder\n",
    "        for d in range(depth):\n",
    "            filters //= 2\n",
    "            if bilinear:\n",
    "                up = nn.Sequential(\n",
    "                    nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "                    nn.Conv2d(filters * 2, filters, kernel_size=1)\n",
    "                )\n",
    "            else:\n",
    "                up = nn.ConvTranspose2d(filters * 2, filters, kernel_size=2, stride=2)\n",
    "            self.up_layers.append(nn.ModuleDict({\n",
    "                'up': up,\n",
    "                'conv': DoubleConv(filters * 2, filters)\n",
    "            }))\n",
    "\n",
    "        # Output layer\n",
    "        self.out_conv = nn.Conv2d(init_filters, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        for down in self.down_layers:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            skip = skip_connections[-(i+1)]\n",
    "            up = self.up_layers[i]['up'](x)\n",
    "            if up.size() != skip.size():\n",
    "                # Resize in case of odd size mismatch\n",
    "                up = F.interpolate(up, size=skip.shape[2:])\n",
    "            x = torch.cat([skip, up], dim=1)\n",
    "            x = self.up_layers[i]['conv'](x)\n",
    "\n",
    "        return self.out_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8c0567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric: Dice Coefficient\n",
    "def dice_coefficient(pred, target, smooth=1e-5):\n",
    "    # Pred is raw logits, apply sigmoid\n",
    "    pred = torch.sigmoid(pred)\n",
    "    # Binarize\n",
    "    pred = (pred > 0.5).float()\n",
    "    \n",
    "    intersection = (pred * target).sum()\n",
    "    dice = (2. * intersection + smooth) / (pred.sum() + target.sum() + smooth)\n",
    "    return dice\n",
    "\n",
    "# Initialize Model\n",
    "model = UNet(in_channels=3, out_channels=1, init_filters=32, depth=4).to(device)\n",
    "\n",
    "# Loss Function: BCEWithLogitsLoss is standard for binary segmentation\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"Model initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad79539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "val_dices = []\n",
    "\n",
    "best_dice = 0.0\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Training Phase\n",
    "    for images, masks in tqdm(train_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS} - Train', leave=False):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    epoch_train_loss = running_loss / len(train_dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    \n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    running_dice = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(val_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS} - Val', leave=False):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            running_val_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            # Compute Dice\n",
    "            running_dice += dice_coefficient(outputs, masks).item() * images.size(0)\n",
    "            \n",
    "    epoch_val_loss = running_val_loss / len(val_dataset)\n",
    "    epoch_dice = running_dice / len(val_dataset)\n",
    "    \n",
    "    val_losses.append(epoch_val_loss)\n",
    "    val_dices.append(epoch_dice)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {epoch_train_loss:.4f} | Val Loss: {epoch_val_loss:.4f} | Val Dice: {epoch_dice:.4f}\")\n",
    "    \n",
    "    # Save Best Model\n",
    "    if epoch_dice > best_dice:\n",
    "        best_dice = epoch_dice\n",
    "        torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, 'best_model.pth'))\n",
    "        print(f\"--> Best model saved with Dice: {best_dice:.4f}\")\n",
    "\n",
    "    # Save Checkpoint periodically\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, f'model_epoch_{epoch+1}.pth'))\n",
    "\n",
    "# Save training parameters for future inference\n",
    "params = {\n",
    "    \"input_size\": INPUT_SIZE,\n",
    "    \"in_channels\": 3,\n",
    "    \"out_channels\": 1,\n",
    "    \"init_filters\": 32,\n",
    "    \"depth\": 4,\n",
    "    \"best_dice\": best_dice\n",
    "}\n",
    "with open(os.path.join(CHECKPOINT_DIR, 'training_params.json'), 'w') as f:\n",
    "    json.dump(params, f, indent=4)\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0667b742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Metrics\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss', marker='o')\n",
    "plt.plot(val_losses, label='Validation Loss', marker='o')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss (BCE)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(val_dices, label='Validation Dice', color='green', marker='o')\n",
    "plt.title('Dice Score over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Dice Coefficient')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualize Predictions on Validation Set\n",
    "model.load_state_dict(torch.load(os.path.join(CHECKPOINT_DIR, 'best_model.pth')))\n",
    "model.eval()\n",
    "\n",
    "images, masks = next(iter(val_loader))\n",
    "images = images.to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    preds = torch.sigmoid(outputs) > 0.5\n",
    "\n",
    "# Convert to numpy for plotting\n",
    "images_np = images.cpu().permute(0, 2, 3, 1).numpy()\n",
    "masks_np = masks.cpu().squeeze().numpy()\n",
    "preds_np = preds.cpu().squeeze().numpy()\n",
    "\n",
    "n_plot = min(3, BATCH_SIZE)\n",
    "plt.figure(figsize=(15, 5 * n_plot))\n",
    "\n",
    "for i in range(n_plot):\n",
    "    plt.subplot(n_plot, 3, i*3 + 1)\n",
    "    plt.imshow(images_np[i])\n",
    "    plt.title(f\"Original Image {i+1}\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(n_plot, 3, i*3 + 2)\n",
    "    plt.imshow(masks_np[i], cmap='gray')\n",
    "    plt.title(f\"Ground Truth {i+1}\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(n_plot, 3, i*3 + 3)\n",
    "    plt.imshow(preds_np[i], cmap='gray')\n",
    "    plt.title(f\"Prediction {i+1}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
