{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68549992",
   "metadata": {},
   "source": [
    "# Steatosis Segmentation using U-Net\n",
    "Training a U-Net model to segment liver steatosis using the provided Training and Validation datasets.\n",
    "This notebook implements the training pipeline including data loading, model definition, training loop, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dde62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Configuration\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 20\n",
    "INPUT_SIZE = (256, 256)\n",
    "CHECKPOINT_DIR = 'checkpoints_steatosis'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Data paths (Modify these if your folder structure is different)\n",
    "TRAIN_DIR = '/content/SteatosisU-UNet/train'\n",
    "VAL_DIR = '/content/SteatosisU-UNet/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f901b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteatosisDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with 'image' and 'manual' subdirs.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.image_dir = os.path.join(root_dir, 'image')\n",
    "        # MODIFICA: Usiamo la cartella corretta per il training\n",
    "        self.mask_dir = os.path.join(root_dir, 'manual_py')\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load file lists\n",
    "        # We filter for common image extensions\n",
    "        valid_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.tif')\n",
    "        self.images = sorted([f for f in os.listdir(self.image_dir) if f.lower().endswith(valid_extensions)])\n",
    "        self.masks = sorted([f for f in os.listdir(self.mask_dir) if f.lower().endswith(valid_extensions)])\n",
    "        \n",
    "        # Basic check\n",
    "        if len(self.images) != len(self.masks):\n",
    "            print(f\"Warning: Number of images ({len(self.images)}) and masks ({len(self.masks)}) in {root_dir} do not match!\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        mask_name = self.masks[idx] \n",
    "        \n",
    "        # Construct full paths\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
    "        \n",
    "        # Load image and mask\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\") # Grayscale for mask\n",
    "        \n",
    "        # Apply transforms\n",
    "        \n",
    "        # Resize image (Bilineare va bene per le foto)\n",
    "        resize_img = transforms.Resize(INPUT_SIZE, interpolation=transforms.InterpolationMode.BILINEAR)\n",
    "        image = resize_img(image)\n",
    "        \n",
    "        # Resize mask (NEAREST è fondamentale per le maschere per mantenere 0 e 1 puri)\n",
    "        resize_mask = transforms.Resize(INPUT_SIZE, interpolation=transforms.InterpolationMode.NEAREST)\n",
    "        mask = resize_mask(mask)\n",
    "        \n",
    "        to_tensor = transforms.ToTensor()\n",
    "        \n",
    "        image = to_tensor(image)\n",
    "        mask = to_tensor(mask)\n",
    "        \n",
    "        # Con manual_py i valori sono spesso molto bassi (0 e 1 su scala 255 vengono caricati quasi neri).\n",
    "        # ToTensor normalizza in [0, 1]. Se il pixel era 1 (classe steatosi), diventa 1/255.\n",
    "        # Se invece manual_py è salvata come 0 e 255, diventa 0 e 1.\n",
    "        # Per sicurezza assoluta, binarizziamo qualunque cosa > 0.\n",
    "        mask = (mask > 0).float()\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c069690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dataset instances\n",
    "train_dataset = SteatosisDataset(TRAIN_DIR)\n",
    "val_dataset = SteatosisDataset(VAL_DIR)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Training set: {len(train_dataset)} images\")\n",
    "print(f\"Validation set: {len(val_dataset)} images\")\n",
    "\n",
    "# Sanity check: Visualize one sample\n",
    "temp_img, temp_mask = train_dataset[0]\n",
    "print(f\"Image tensor shape: {temp_img.shape}\")\n",
    "print(f\"Mask tensor shape: {temp_mask.shape}\")\n",
    "print(f\"Mask unique values: {torch.unique(temp_mask)}\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(temp_img.permute(1, 2, 0))\n",
    "plt.title(\"Sample Image\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(temp_mask.squeeze(), cmap='gray')\n",
    "plt.title(\"Sample Mask\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdd012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"Applies two consecutive conv-batchnorm-relu layers\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels=3,\n",
    "                 out_channels=1,\n",
    "                 init_filters=64,\n",
    "                 depth=4,\n",
    "                 bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.down_layers = nn.ModuleList()\n",
    "        self.up_layers = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # Encoder\n",
    "        filters = init_filters\n",
    "        for d in range(depth):\n",
    "            conv = DoubleConv(in_channels, filters)\n",
    "            self.down_layers.append(conv)\n",
    "            in_channels = filters\n",
    "            filters *= 2\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv(in_channels, filters)\n",
    "\n",
    "        # Decoder\n",
    "        for d in range(depth):\n",
    "            filters //= 2\n",
    "            if bilinear:\n",
    "                up = nn.Sequential(\n",
    "                    nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "                    nn.Conv2d(filters * 2, filters, kernel_size=1)\n",
    "                )\n",
    "            else:\n",
    "                up = nn.ConvTranspose2d(filters * 2, filters, kernel_size=2, stride=2)\n",
    "            self.up_layers.append(nn.ModuleDict({\n",
    "                'up': up,\n",
    "                'conv': DoubleConv(filters * 2, filters)\n",
    "            }))\n",
    "\n",
    "        # Output layer\n",
    "        self.out_conv = nn.Conv2d(init_filters, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        for down in self.down_layers:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            skip = skip_connections[-(i+1)]\n",
    "            up = self.up_layers[i]['up'](x)\n",
    "            if up.size() != skip.size():\n",
    "                # Resize in case of odd size mismatch\n",
    "                up = F.interpolate(up, size=skip.shape[2:])\n",
    "            x = torch.cat([skip, up], dim=1)\n",
    "            x = self.up_layers[i]['conv'](x)\n",
    "\n",
    "        return self.out_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8c0567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric Functions\n",
    "def compute_batch_stats(pred_logits, target_mask):\n",
    "    \"\"\"\n",
    "    Calculates statistics for a batch to compute:\n",
    "    1. Dice Standard (Mean per Image)\n",
    "    2. Dice Strict (Mean per Image)\n",
    "    3. Global Stats (Intersection & Union) for Batch-Based/Global Dice\n",
    "    \"\"\"\n",
    "    # Sigmoid & Binarization\n",
    "    probs = torch.sigmoid(pred_logits)\n",
    "    pred = (probs > 0.5).float()\n",
    "    \n",
    "    # Flatten: (B, C, H, W) -> (B, -1)\n",
    "    pred_flat = pred.view(pred.size(0), -1)\n",
    "    target_flat = target_mask.view(target_mask.size(0), -1)\n",
    "    \n",
    "    # Intersection & Sum per image\n",
    "    intersection = (pred_flat * target_flat).sum(dim=1)\n",
    "    union_raw = pred_flat.sum(dim=1) + target_flat.sum(dim=1)\n",
    "    \n",
    "    # --- 1. Dice Standard Per Image (with smooth) ---\n",
    "    smooth = 1e-5\n",
    "    dice_std_img = (2. * intersection + smooth) / (union_raw + smooth)\n",
    "    \n",
    "    # --- 2. Dice Strict Per Image (no smooth, handle empty) ---\n",
    "    # If both empty (union=0) -> 1.0, Else calc dice.\n",
    "    dice_strict_img = torch.zeros_like(intersection)\n",
    "    \n",
    "    is_empty = (union_raw == 0)\n",
    "    # Case both empty -> 1\n",
    "    dice_strict_img[is_empty] = 1.0 \n",
    "    # Case not empty -> 2*I / U\n",
    "    if (~is_empty).any():\n",
    "        dice_strict_img[~is_empty] = (2. * intersection[~is_empty]) / union_raw[~is_empty]\n",
    "\n",
    "    # --- 3. For Global/Batch Calculation ---\n",
    "    total_intersection = intersection.sum().item()\n",
    "    total_union = union_raw.sum().item()\n",
    "    \n",
    "    return {\n",
    "        'sum_dice_std': dice_std_img.sum().item(),\n",
    "        'sum_dice_strict': dice_strict_img.sum().item(),\n",
    "        'total_int': total_intersection,\n",
    "        'total_union': total_union,\n",
    "        'n_samples': pred.size(0)\n",
    "    }\n",
    "\n",
    "# Initialize Model\n",
    "model = UNet(in_channels=3, out_channels=1, init_filters=32, depth=4).to(device)\n",
    "\n",
    "# Loss Function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"Model initialized with detailed metric tracking.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad79539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# History storage\n",
    "history = {\n",
    "    'train_loss': [], 'val_loss': [],\n",
    "    'train_dice_std_avg': [], 'val_dice_std_avg': [],\n",
    "    'train_dice_strict_avg': [], 'val_dice_strict_avg': [],\n",
    "    'train_dice_std_global': [], 'val_dice_std_global': [],\n",
    "    'train_dice_strict_global': [], 'val_dice_strict_global': []\n",
    "}\n",
    "\n",
    "best_metric = 0.0\n",
    "\n",
    "print(f\"Starting training for {NUM_EPOCHS} epochs...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # --- TRAINING ---\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Accumulators for metrics\n",
    "    train_stats = {'sum_dice_std': 0, 'sum_dice_strict': 0, 'total_int': 0, 'total_union': 0, 'n_samples': 0}\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS} - Train', leave=False)\n",
    "    for images, masks in pbar:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        # Calculate stats for this batch\n",
    "        with torch.no_grad():\n",
    "            batch_s = compute_batch_stats(outputs, masks)\n",
    "            for k in train_stats:\n",
    "                train_stats[k] += batch_s[k]\n",
    "                \n",
    "    # End of Train Epoch Calculations\n",
    "    epoch_train_loss = running_loss / len(train_dataset)\n",
    "    \n",
    "    # Train Metrics\n",
    "    t_dice_std_avg = train_stats['sum_dice_std'] / train_stats['n_samples']\n",
    "    t_dice_strict_avg = train_stats['sum_dice_strict'] / train_stats['n_samples']\n",
    "    \n",
    "    # Global Train Metrics\n",
    "    smooth = 1e-5\n",
    "    t_dice_std_global = (2. * train_stats['total_int'] + smooth) / (train_stats['total_union'] + smooth)\n",
    "    \n",
    "    if train_stats['total_union'] == 0:\n",
    "         # If the whole dataset was empty? Unlikely, but let's handle.\n",
    "         # If int is also 0 -> 1.\n",
    "         t_dice_strict_global = 1.0 if train_stats['total_int'] == 0 else 0.0\n",
    "    else:\n",
    "         t_dice_strict_global = (2. * train_stats['total_int']) / train_stats['total_union']\n",
    "\n",
    "    # --- VALIDATION ---\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    val_stats = {'sum_dice_std': 0, 'sum_dice_strict': 0, 'total_int': 0, 'total_union': 0, 'n_samples': 0}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(val_loader, desc=f'Epoch {epoch+1}/{NUM_EPOCHS} - Val', leave=False):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            running_val_loss += loss.item() * images.size(0)\n",
    "            \n",
    "            batch_s = compute_batch_stats(outputs, masks)\n",
    "            for k in val_stats:\n",
    "                val_stats[k] += batch_s[k]\n",
    "\n",
    "    # End of Val Epoch Calculations\n",
    "    epoch_val_loss = running_val_loss / len(val_dataset)\n",
    "    \n",
    "    # Val Metrics\n",
    "    v_dice_std_avg = val_stats['sum_dice_std'] / val_stats['n_samples']\n",
    "    v_dice_strict_avg = val_stats['sum_dice_strict'] / val_stats['n_samples']\n",
    "    \n",
    "    # Global Val Metrics\n",
    "    v_dice_std_global = (2. * val_stats['total_int'] + smooth) / (val_stats['total_union'] + smooth)\n",
    "    \n",
    "    if val_stats['total_union'] == 0:\n",
    "         v_dice_strict_global = 1.0 if val_stats['total_int'] == 0 else 0.0\n",
    "    else:\n",
    "         v_dice_strict_global = (2. * val_stats['total_int']) / val_stats['total_union']\n",
    "\n",
    "    # Update History\n",
    "    history['train_loss'].append(epoch_train_loss)\n",
    "    history['val_loss'].append(epoch_val_loss)\n",
    "    history['train_dice_std_avg'].append(t_dice_std_avg)\n",
    "    history['val_dice_std_avg'].append(v_dice_std_avg)\n",
    "    history['train_dice_strict_avg'].append(t_dice_strict_avg)\n",
    "    history['val_dice_strict_avg'].append(v_dice_strict_avg)\n",
    "    history['train_dice_std_global'].append(t_dice_std_global)\n",
    "    history['val_dice_std_global'].append(v_dice_std_global)\n",
    "    history['train_dice_strict_global'].append(t_dice_strict_global)\n",
    "    history['val_dice_strict_global'].append(v_dice_strict_global)\n",
    "\n",
    "    # Print Report\n",
    "    print(f\"Epoch {epoch+1:02d} | Loss: T={epoch_train_loss:.4f} V={epoch_val_loss:.4f}\")\n",
    "    print(f\"   [Per-Image]  Dice Strict: T={t_dice_strict_avg:.4f} V={v_dice_strict_avg:.4f} | Std: T={t_dice_std_avg:.4f} V={v_dice_std_avg:.4f}\")\n",
    "    print(f\"   [Batch-Based] Dice Strict: T={t_dice_strict_global:.4f} V={v_dice_strict_global:.4f} | Std: T={t_dice_std_global:.4f} V={v_dice_std_global:.4f}\")\n",
    "\n",
    "    # Save Checkpoints\n",
    "    current_metric = v_dice_strict_avg  # Use Per-Image Strict Dice as main criteria\n",
    "    if current_metric > best_metric:\n",
    "        best_metric = current_metric\n",
    "        torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, 'best_model.pth'))\n",
    "        print(f\"   --> NEW BEST MODEL (Dice Strict Avg: {best_metric:.4f})\")\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        torch.save(model.state_dict(), os.path.join(CHECKPOINT_DIR, f'model_epoch_{epoch+1}.pth'))\n",
    "\n",
    "# Save final params\n",
    "params = {\n",
    "    \"config\": {\"batch_size\": BATCH_SIZE, \"epochs\": NUM_EPOCHS, \"input_size\": INPUT_SIZE},\n",
    "    \"results\": {\n",
    "        \"best_val_dice_strict_avg\": best_metric,\n",
    "        \"final_val_dice_strict_global\": v_dice_strict_global\n",
    "    }\n",
    "}\n",
    "with open(os.path.join(CHECKPOINT_DIR, 'training_results.json'), 'w') as f:\n",
    "    json.dump(params, f, indent=4)\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0667b742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Comprehensive Metrics\n",
    "plt.figure(figsize=(18, 10))\n",
    "\n",
    "# 1. Losses\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(history['train_loss'], label='Train Loss', marker='.')\n",
    "plt.plot(history['val_loss'], label='Val Loss', marker='.')\n",
    "plt.title('BCE Loss')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# 2. Strict Dice (Per Image - The main metric)\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(history['train_dice_strict_avg'], label='Train Strict (Avg)', marker='.')\n",
    "plt.plot(history['val_dice_strict_avg'], label='Val Strict (Avg)', marker='.')\n",
    "plt.title('Dice Strict (Per-Image Average)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# 3. Global vs Average Comparison (Validation Only)\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(history['val_dice_strict_avg'], label='Val Strict (Avg)', marker='.', linestyle='--')\n",
    "plt.plot(history['val_dice_strict_global'], label='Val Strict (Global)', marker='.')\n",
    "plt.title('Validation: Average vs Global (Strict)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# 4. Standard vs Strict Comparison (Validation Only)\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(history['val_dice_std_avg'], label='Val Standard (Avg)', marker='.', linestyle='--')\n",
    "plt.plot(history['val_dice_strict_avg'], label='Val Strict (Avg)', marker='.')\n",
    "plt.title('Validation: Standard vs Strict (Average)')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Visualization of Predictions (Best Model) ---\n",
    "model.load_state_dict(torch.load(os.path.join(CHECKPOINT_DIR, 'best_model.pth')))\n",
    "model.eval()\n",
    "\n",
    "# Get a batch\n",
    "images, masks = next(iter(val_loader))\n",
    "images = images.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    preds = torch.sigmoid(outputs) > 0.5 \n",
    "\n",
    "# Plot\n",
    "n_plot = min(3, images.size(0))\n",
    "fig, axs = plt.subplots(n_plot, 3, figsize=(15, 5*n_plot))\n",
    "\n",
    "images_np = images.cpu().permute(0, 2, 3, 1).numpy()\n",
    "masks_np = masks.cpu().squeeze().numpy()\n",
    "preds_np = preds.float().cpu().squeeze().numpy()\n",
    "\n",
    "if n_plot == 1: axs = [axs] # Handle single case\n",
    "\n",
    "for i in range(n_plot):\n",
    "    # Original\n",
    "    axs[i][0].imshow(images_np[i])\n",
    "    axs[i][0].set_title(f\"Image {i+1}\")\n",
    "    axs[i][0].axis('off')\n",
    "    \n",
    "    # Ground Truth\n",
    "    axs[i][1].imshow(masks_np[i], cmap='gray', vmin=0, vmax=1)\n",
    "    axs[i][1].set_title(\"Ground Truth (Manual)\")\n",
    "    axs[i][1].axis('off')\n",
    "    \n",
    "    # Prediction\n",
    "    axs[i][2].imshow(preds_np[i], cmap='gray', vmin=0, vmax=1)\n",
    "    axs[i][2].set_title(\"Prediction (U-Net)\")\n",
    "    axs[i][2].axis('off')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b72b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# VISUALIZZAZIONE E SALVATAGGIO DI TUTTE LE PREDIZIONI DEL VALIDATION SET\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "OUTPUT_DIR = 'val_predictions_output'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 1. Carichiamo il modello migliore assoluto\n",
    "print(\"Caricamento del modello migliore...\")\n",
    "model.load_state_dict(torch.load(os.path.join(CHECKPOINT_DIR, 'best_model.pth')))\n",
    "model.eval()\n",
    "\n",
    "# Liste per lo storage temporaneo dei dati da plottare\n",
    "plot_data = []\n",
    "\n",
    "print(f\"Salvataggio maschere in: {os.path.abspath(OUTPUT_DIR)}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Iteriamo direttamente sul dataset per avere accesso facile ai nomi dei file originali\n",
    "    # Nota: val_dataset.images contiene la lista dei nomi dei file\n",
    "    for i in range(len(val_dataset)):\n",
    "        \n",
    "        # Recuperiamo i dati dal dataset\n",
    "        img_tensor, mask_tensor = val_dataset[i]\n",
    "        filename = val_dataset.images[i]\n",
    "        \n",
    "        # Preparazione input per la rete (aggiungiamo dimensione batch: [1, 3, 256, 256])\n",
    "        img_input = img_tensor.unsqueeze(0).to(device)\n",
    "        \n",
    "        # INFERENCE\n",
    "        output = model(img_input)\n",
    "        \n",
    "        # Post-processing: Sigmoide -> Soglia 0.5 -> float\n",
    "        pred_prob = torch.sigmoid(output)\n",
    "        pred_mask = (pred_prob > 0.5).float().cpu().squeeze().numpy() # [256, 256]\n",
    "        \n",
    "        # Recuperiamo la ground truth per il plot\n",
    "        gt_mask = mask_tensor.squeeze().numpy() # [256, 256]\n",
    "        \n",
    "        # Recuperiamo l'immagine originale per il plot (da tensore a numpy HWC)\n",
    "        orig_img = img_tensor.permute(1, 2, 0).numpy()\n",
    "        \n",
    "        # --- SALVATAGGIO SU DISCO ---\n",
    "        # Convertiamo la maschera da 0.0-1.0 a 0-255 uint8 per salvarla come immagine visibile\n",
    "        pred_img_pil = Image.fromarray((pred_mask * 255).astype(np.uint8))\n",
    "        save_path = os.path.join(OUTPUT_DIR, filename)\n",
    "        pred_img_pil.save(save_path)\n",
    "        \n",
    "        # Aggiungiamo alla lista per il plot finale\n",
    "        plot_data.append({\n",
    "            'filename': filename,\n",
    "            'orig': orig_img,\n",
    "            'gt': gt_mask,\n",
    "            'pred': pred_mask\n",
    "        })\n",
    "\n",
    "print(f\"Generate {len(plot_data)} maschere.\")\n",
    "\n",
    "# --- PLOT DI TUTTE LE IMMAGINI ---\n",
    "# Attenzione: se il validation set è enorme, questa immagine sarà molto alta verticale.\n",
    "num_samples = len(plot_data)\n",
    "fig, axes = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))\n",
    "\n",
    "print(\"Generazione grafico comparativo...\")\n",
    "\n",
    "# Gestione caso speciale se c'è solo 1 immagine nel validation set\n",
    "if num_samples == 1:\n",
    "    axes = np.expand_dims(axes, axis=0)\n",
    "\n",
    "for idx, data in enumerate(plot_data):\n",
    "    # Colonna 1: Immagine Originale\n",
    "    axes[idx, 0].imshow(data['orig'])\n",
    "    axes[idx, 0].set_title(f\"Orig: {data['filename']}\")\n",
    "    axes[idx, 0].axis('off')\n",
    "    \n",
    "    # Colonna 2: Maschera Manuale (Ground Truth)\n",
    "    axes[idx, 1].imshow(data['gt'], cmap='gray')\n",
    "    axes[idx, 1].set_title(\"Manual (Verità)\")\n",
    "    axes[idx, 1].axis('off')\n",
    "    \n",
    "    # Colonna 3: Maschera Predetta dalla U-Net\n",
    "    axes[idx, 2].imshow(data['pred'], cmap='gray')\n",
    "    axes[idx, 2].set_title(\"Predizione U-Net\")\n",
    "    axes[idx, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
