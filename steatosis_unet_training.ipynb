{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68549992",
   "metadata": {},
   "source": [
    "# Steatosis Segmentation using U-Net\n",
    "Training a U-Net model to segment liver steatosis using the provided Training and Validation datasets.\n",
    "This notebook implements the training pipeline including data loading, model definition, training loop, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dde62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Configuration\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 1e-4\n",
    "NUM_EPOCHS = 20\n",
    "INPUT_SIZE = (256, 256)\n",
    "CHECKPOINT_DIR = 'checkpoints_steatosis'\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Data paths (Modify these if your folder structure is different)\n",
    "TRAIN_DIR = '/content/SteatosisU-UNet/train'\n",
    "VAL_DIR = '/content/SteatosisU-UNet/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f901b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SteatosisDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, normalization_method=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with 'image' and 'manual' subdirs.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            normalization_method (str): None, 'reinhard', or 'macenko'.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.image_dir = os.path.join(root_dir, 'image')\n",
    "        # Check correct folder\n",
    "        if os.path.exists(os.path.join(root_dir, 'manual_py')):\n",
    "            self.mask_dir = os.path.join(root_dir, 'manual_py')\n",
    "        else:\n",
    "            self.mask_dir = os.path.join(root_dir, 'manual')\n",
    "            print(f\"Warning: 'manual_py' not found in {root_dir}. Using 'manual'.\")\n",
    "\n",
    "        self.transform = transform\n",
    "        self.normalization_method = normalization_method\n",
    "        \n",
    "        # Load file lists\n",
    "        valid_extensions = ('.png', '.jpg', '.jpeg', '.bmp', '.tif')\n",
    "        self.images = sorted([f for f in os.listdir(self.image_dir) if f.lower().endswith(valid_extensions)])\n",
    "        self.masks = sorted([f for f in os.listdir(self.mask_dir) if f.lower().endswith(valid_extensions)])\n",
    "        \n",
    "        if len(self.images) != len(self.masks):\n",
    "            print(f\"Warning: Mismatch between images and masks in {root_dir}\")\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        mask_name = self.masks[idx] \n",
    "        \n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        mask_path = os.path.join(self.mask_dir, mask_name)\n",
    "        \n",
    "        # Load\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path).convert(\"L\")\n",
    "        \n",
    "        # Apply Normalization BEFORE Transforms (requires numpy)\n",
    "        if self.normalization_method:\n",
    "            img_np = np.array(image)\n",
    "            try:\n",
    "                norm_np = img_np\n",
    "                if self.normalization_method == 'reinhard':\n",
    "                    norm_np = reinhard_normalize(img_np)\n",
    "                elif self.normalization_method == 'macenko':\n",
    "                     # Placeholder if macenko is still available, otherwise ignore\n",
    "                     # norm_np = macenko_normalize(img_np) \n",
    "                     pass\n",
    "                     \n",
    "                image = Image.fromarray(norm_np)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        # Transforms\n",
    "        # Resize image\n",
    "        resize_img = transforms.Resize(INPUT_SIZE, interpolation=transforms.InterpolationMode.BILINEAR)\n",
    "        image = resize_img(image)\n",
    "        \n",
    "        # Resize mask\n",
    "        # IMPORTANT: Nearest Neighbor for masks\n",
    "        resize_mask = transforms.Resize(INPUT_SIZE, interpolation=transforms.InterpolationMode.NEAREST)\n",
    "        mask = resize_mask(mask)\n",
    "        \n",
    "        # ToTensor\n",
    "        to_tensor = transforms.ToTensor()\n",
    "        image = to_tensor(image)\n",
    "        mask_t = to_tensor(mask)\n",
    "        \n",
    "        # Binarize Mask (Hard threshold 0)\n",
    "        # Assuming manual_py is 0/1 or 0/255.\n",
    "        mask_t = (mask_t > 0).float()\n",
    "        \n",
    "        return image, mask_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3eaf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinhard Normalization Implementation\n",
    "def reinhard_normalize(source_img, target_mu=None, target_sigma=None):\n",
    "    \"\"\"\n",
    "    Normalizes an image using Reinhard's method.\n",
    "    source_img: Input image (PIL or numpy array, RGB, uint8)\n",
    "    target_mu, target_sigma: Statistics of target image (in LAB space)\n",
    "    Reference: Reinhard et al. 2001\n",
    "    \"\"\"\n",
    "    source_img = np.array(source_img)\n",
    "    \n",
    "    # Defaults (from a reference \"good\" slide)\n",
    "    if target_mu is None:\n",
    "        target_mu = np.array([8.6323, -0.1150, 0.0387]) # L, A, B means\n",
    "    if target_sigma is None:\n",
    "        target_sigma = np.array([0.5750, 0.1040, 0.0136]) # L, A, B stds\n",
    "\n",
    "    # RGB to LAB (using simple cv2-like conversion or skimage)\n",
    "    # Using simplified conversion for dependency-free numpy\n",
    "    \n",
    "    # 1. RGB to LMS\n",
    "    source_img = source_img.astype(float)\n",
    "    # Add small epsilon to avoid log(0)\n",
    "    source_img = np.maximum(source_img, 1e-5)\n",
    "    \n",
    "    M_rgb_lms = np.array([\n",
    "        [0.3811, 0.5783, 0.0402],\n",
    "        [0.1967, 0.7244, 0.0782],\n",
    "        [0.0241, 0.1288, 0.8444]\n",
    "    ])\n",
    "    \n",
    "    lms = np.dot(source_img.reshape(-1, 3), M_rgb_lms.T)\n",
    "    lms = np.log10(lms)\n",
    "    \n",
    "    # 2. LMS to LAB\n",
    "    M_lms_lab = np.array([\n",
    "        [1/np.sqrt(3), 0, 0],\n",
    "        [0, 1/np.sqrt(6), 0],\n",
    "        [0, 0, 1/np.sqrt(2)]\n",
    "    ]) @ np.array([\n",
    "        [1, 1, 1],\n",
    "        [1, 1, -2],\n",
    "        [1, -1, 0]\n",
    "    ])\n",
    "    \n",
    "    lab = np.dot(lms, M_lms_lab.T)\n",
    "    \n",
    "    # 3. Statistics\n",
    "    mu = np.mean(lab, axis=0)\n",
    "    sigma = np.std(lab, axis=0)\n",
    "    \n",
    "    # 4. Normalize\n",
    "    # Clip very small sigma to avoid extreme scaling\n",
    "    sigma = np.maximum(sigma, 1e-5)\n",
    "    \n",
    "    lab_norm = (lab - mu) * (target_sigma / sigma) + target_mu\n",
    "    \n",
    "    # 5. LAB to LMS\n",
    "    lms_norm = np.dot(lab_norm, np.linalg.inv(M_lms_lab).T)\n",
    "    lms_norm = np.power(10, lms_norm)\n",
    "    \n",
    "    # 6. LMS to RGB\n",
    "    rgb_norm = np.dot(lms_norm, np.linalg.inv(M_rgb_lms).T)\n",
    "    rgb_norm = np.clip(rgb_norm, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    return rgb_norm.reshape(source_img.shape[0], source_img.shape[1], 3)\n",
    "\n",
    "def visualize_reinhard_demo(dataset, num_samples=5):\n",
    "    \"\"\"\n",
    "    Visualizes original vs reinhard normalized images.\n",
    "    \"\"\"\n",
    "    indices = np.random.choice(len(dataset), size=min(len(dataset), num_samples), replace=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 3 * num_samples))\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        img_path = os.path.join(dataset.image_dir, dataset.images[idx])\n",
    "        original = Image.open(img_path).convert(\"RGB\")\n",
    "        original_np = np.array(original)\n",
    "        \n",
    "        try:\n",
    "            # Force reinhard regardless of dataset setting\n",
    "            normalized_np = reinhard_normalize(original_np)\n",
    "            \n",
    "            plt.subplot(num_samples, 2, i*2 + 1)\n",
    "            plt.imshow(original_np)\n",
    "            plt.title(f\"Original {idx}\")\n",
    "            plt.axis('off')\n",
    "            \n",
    "            plt.subplot(num_samples, 2, i*2 + 2)\n",
    "            plt.imshow(normalized_np)\n",
    "            plt.title(\"Reinhard Normalized\")\n",
    "            plt.axis('off')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error normalizing {idx}: {e}\")\n",
    "            \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"Reinhard Normalizer Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c069690e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Visualization of Reinhard Normalization (Before Training)\n",
    "print(\"Generating Reinhard Normalization Previews...\")\n",
    "# Create a temporary dataset just for visualization\n",
    "temp_viz_dataset_train = SteatosisDataset(TRAIN_DIR, normalization_method=None)\n",
    "temp_viz_dataset_val = SteatosisDataset(VAL_DIR, normalization_method=None)\n",
    "\n",
    "# Save images directory\n",
    "VIZ_OUTPUT_DIR = 'normalization_previews_reinhard'\n",
    "os.makedirs(VIZ_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 20 samples from Train\n",
    "print(\"Visualizing 20 Train samples...\")\n",
    "subset_indices = np.random.choice(len(temp_viz_dataset_train), 20, replace=False)\n",
    "\n",
    "fig, axs = plt.subplots(20, 2, figsize=(8, 60))\n",
    "for i, idx in enumerate(subset_indices):\n",
    "    img_name = temp_viz_dataset_train.images[idx]\n",
    "    img_path = os.path.join(temp_viz_dataset_train.image_dir, img_name)\n",
    "    \n",
    "    orig = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "    norm = reinhard_normalize(orig)\n",
    "    \n",
    "    # Plot\n",
    "    axs[i, 0].imshow(orig)\n",
    "    axs[i, 0].set_title(f\"Train Orig: {img_name}\")\n",
    "    axs[i, 0].axis('off')\n",
    "    \n",
    "    axs[i, 1].imshow(norm)\n",
    "    axs[i, 1].set_title(\"Reinhard Norm\")\n",
    "    axs[i, 1].axis('off')\n",
    "    \n",
    "    # Save file\n",
    "    Image.fromarray(norm).save(os.path.join(VIZ_OUTPUT_DIR, f\"norm_train_{img_name}\"))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# All samples from Val\n",
    "print(f\"Visualizing ALL ({len(temp_viz_dataset_val)}) Val samples...\")\n",
    "fig_v, axs_v = plt.subplots(len(temp_viz_dataset_val), 2, figsize=(8, 3 * len(temp_viz_dataset_val)))\n",
    "if len(temp_viz_dataset_val) == 1: axs_v = np.expand_dims(axs_v, 0)\n",
    "\n",
    "for i in range(len(temp_viz_dataset_val)):\n",
    "    img_name = temp_viz_dataset_val.images[i]\n",
    "    img_path = os.path.join(temp_viz_dataset_val.image_dir, img_name)\n",
    "    \n",
    "    orig = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "    norm = reinhard_normalize(orig)\n",
    "    \n",
    "    # Plot\n",
    "    axs_v[i, 0].imshow(orig)\n",
    "    axs_v[i, 0].set_title(f\"Val Orig: {img_name}\")\n",
    "    axs_v[i, 0].axis('off')\n",
    "    \n",
    "    axs_v[i, 1].imshow(norm)\n",
    "    axs_v[i, 1].set_title(\"Reinhard Norm\")\n",
    "    axs_v[i, 1].axis('off')\n",
    "    \n",
    "    # Save file\n",
    "    Image.fromarray(norm).save(os.path.join(VIZ_OUTPUT_DIR, f\"norm_val_{img_name}\"))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(f\"Normalized images saved to {VIZ_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de068281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. DEFINITION OF TRAINING FUNCTION\n",
    "# Wrapping the training loop in a function to run it twice (Baseline vs Reinhard)\n",
    "def run_training_experiment(experiment_name, train_dataset, val_dataset):\n",
    "    print(f\"\\n{'='*20} RUNNING EXPERIMENT: {experiment_name} {'='*20}\")\n",
    "    \n",
    "    # Loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Init Model & Ops\n",
    "    model = UNet(in_channels=3, out_channels=1, init_filters=32, depth=4).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "    # Metrics Storage\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'val_dice_strict_avg': [], 'val_dice_strict_global': []\n",
    "    }\n",
    "    best_metric = 0.0\n",
    "    \n",
    "    save_dir = os.path.join(CHECKPOINT_DIR, experiment_name)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        # TRAIN\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        pbar = tqdm(train_loader, desc=f'[{experiment_name}] Ep {epoch+1}/{NUM_EPOCHS}', leave=False)\n",
    "        \n",
    "        for images, masks in pbar:\n",
    "            images, masks = images.to(device), masks.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            \n",
    "        epoch_train_loss = running_loss / len(train_dataset)\n",
    "        \n",
    "        # VALIDATION\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        val_stats = {'sum_dice_strict': 0, 'total_int': 0, 'total_union': 0, 'n_samples': 0}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images, masks = images.to(device), masks.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                running_val_loss += loss.item() * images.size(0)\n",
    "                \n",
    "                # Metrics\n",
    "                batch_s = compute_batch_stats(outputs, masks)\n",
    "                for k in ['sum_dice_strict', 'total_int', 'total_union', 'n_samples']:\n",
    "                    val_stats[k] += batch_s[k]\n",
    "\n",
    "        epoch_val_loss = running_val_loss / len(val_dataset)\n",
    "        v_strict_avg = val_stats['sum_dice_strict'] / val_stats['n_samples']\n",
    "        v_strict_glob = (2. * val_stats['total_int']) / (val_stats['total_union'] + 1e-7) if val_stats['total_union'] > 0 else 0\n",
    "        \n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['val_dice_strict_avg'].append(v_strict_avg)\n",
    "        history['val_dice_strict_global'].append(v_strict_glob)\n",
    "        \n",
    "        print(f\"[{experiment_name}] Ep {epoch+1} | Loss: {epoch_train_loss:.4f} / {epoch_val_loss:.4f} | Val Dice Strict: {v_strict_avg:.4f}\")\n",
    "        \n",
    "        if v_strict_avg > best_metric:\n",
    "            best_metric = v_strict_avg\n",
    "            torch.save(model.state_dict(), os.path.join(save_dir, 'best_model.pth'))\n",
    "    \n",
    "    # Save History\n",
    "    with open(os.path.join(save_dir, 'history.json'), 'w') as f:\n",
    "        json.dump(history, f)\n",
    "        \n",
    "    return history\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 3. EXECUTE BASELINE (NO NORMALIZATION)\n",
    "# --------------------------------------------------------\n",
    "ds_train_base = SteatosisDataset(TRAIN_DIR, normalization_method=None)\n",
    "ds_val_base = SteatosisDataset(VAL_DIR, normalization_method=None)\n",
    "\n",
    "history_baseline = run_training_experiment(\"BASELINE\", ds_train_base, ds_val_base)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 4. EXECUTE REINHARD (NORMALIZED)\n",
    "# --------------------------------------------------------\n",
    "ds_train_reinhard = SteatosisDataset(TRAIN_DIR, normalization_method='reinhard')\n",
    "ds_val_reinhard = SteatosisDataset(VAL_DIR, normalization_method='reinhard')\n",
    "\n",
    "history_reinhard = run_training_experiment(\"REINHARD\", ds_train_reinhard, ds_val_reinhard)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 5. COMPARISON PLOT\n",
    "# --------------------------------------------------------\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_baseline['val_dice_strict_avg'], label='Baseline', marker='.')\n",
    "plt.plot(history_reinhard['val_dice_strict_avg'], label='Reinhard', marker='.')\n",
    "plt.title(\"Validation Dice Strict (Avg)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_baseline['val_loss'], label='Baseline', marker='.')\n",
    "plt.plot(history_reinhard['val_loss'], label='Reinhard', marker='.')\n",
    "plt.title(\"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdd012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"Applies two consecutive conv-batchnorm-relu layers\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels=3,\n",
    "                 out_channels=1,\n",
    "                 init_filters=64,\n",
    "                 depth=4,\n",
    "                 bilinear=True):\n",
    "        super(UNet, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.down_layers = nn.ModuleList()\n",
    "        self.up_layers = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # Encoder\n",
    "        filters = init_filters\n",
    "        for d in range(depth):\n",
    "            conv = DoubleConv(in_channels, filters)\n",
    "            self.down_layers.append(conv)\n",
    "            in_channels = filters\n",
    "            filters *= 2\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv(in_channels, filters)\n",
    "\n",
    "        # Decoder\n",
    "        for d in range(depth):\n",
    "            filters //= 2\n",
    "            if bilinear:\n",
    "                up = nn.Sequential(\n",
    "                    nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "                    nn.Conv2d(filters * 2, filters, kernel_size=1)\n",
    "                )\n",
    "            else:\n",
    "                up = nn.ConvTranspose2d(filters * 2, filters, kernel_size=2, stride=2)\n",
    "            self.up_layers.append(nn.ModuleDict({\n",
    "                'up': up,\n",
    "                'conv': DoubleConv(filters * 2, filters)\n",
    "            }))\n",
    "\n",
    "        # Output layer\n",
    "        self.out_conv = nn.Conv2d(init_filters, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "        for down in self.down_layers:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "\n",
    "        for i in range(self.depth):\n",
    "            skip = skip_connections[-(i+1)]\n",
    "            up = self.up_layers[i]['up'](x)\n",
    "            if up.size() != skip.size():\n",
    "                # Resize in case of odd size mismatch\n",
    "                up = F.interpolate(up, size=skip.shape[2:])\n",
    "            x = torch.cat([skip, up], dim=1)\n",
    "            x = self.up_layers[i]['conv'](x)\n",
    "\n",
    "        return self.out_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8c0567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric Functions\n",
    "def compute_batch_stats(pred_logits, target_mask):\n",
    "    \"\"\"\n",
    "    Calculates statistics for a batch to compute:\n",
    "    1. Dice Standard (Mean per Image)\n",
    "    2. Dice Strict (Mean per Image)\n",
    "    3. Global Stats (Intersection & Union) for Batch-Based/Global Dice\n",
    "    \"\"\"\n",
    "    # Sigmoid & Binarization\n",
    "    probs = torch.sigmoid(pred_logits)\n",
    "    pred = (probs > 0.5).float()\n",
    "    \n",
    "    # Flatten: (B, C, H, W) -> (B, -1)\n",
    "    pred_flat = pred.view(pred.size(0), -1)\n",
    "    target_flat = target_mask.view(target_mask.size(0), -1)\n",
    "    \n",
    "    # Intersection & Sum per image\n",
    "    intersection = (pred_flat * target_flat).sum(dim=1)\n",
    "    union_raw = pred_flat.sum(dim=1) + target_flat.sum(dim=1)\n",
    "    \n",
    "    # --- 1. Dice Standard Per Image (with smooth) ---\n",
    "    smooth = 1e-5\n",
    "    dice_std_img = (2. * intersection + smooth) / (union_raw + smooth)\n",
    "    \n",
    "    # --- 2. Dice Strict Per Image (no smooth, handle empty) ---\n",
    "    # If both empty (union=0) -> 1.0, Else calc dice.\n",
    "    dice_strict_img = torch.zeros_like(intersection)\n",
    "    \n",
    "    is_empty = (union_raw == 0)\n",
    "    # Case both empty -> 1\n",
    "    dice_strict_img[is_empty] = 1.0 \n",
    "    # Case not empty -> 2*I / U\n",
    "    if (~is_empty).any():\n",
    "        dice_strict_img[~is_empty] = (2. * intersection[~is_empty]) / union_raw[~is_empty]\n",
    "\n",
    "    # --- 3. For Global/Batch Calculation ---\n",
    "    total_intersection = intersection.sum().item()\n",
    "    total_union = union_raw.sum().item()\n",
    "    \n",
    "    return {\n",
    "        'sum_dice_std': dice_std_img.sum().item(),\n",
    "        'sum_dice_strict': dice_strict_img.sum().item(),\n",
    "        'total_int': total_intersection,\n",
    "        'total_union': total_union,\n",
    "        'n_samples': pred.size(0)\n",
    "    }\n",
    "\n",
    "# Initialize Model\n",
    "model = UNet(in_channels=3, out_channels=1, init_filters=32, depth=4).to(device)\n",
    "\n",
    "# Loss Function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"Model initialized with detailed metric tracking.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
