{"cells":[{"cell_type":"markdown","metadata":{"id":"sh6KdaoQIhtR"},"source":["## **Medical Image Processing**\n","### Lab 3 - Deep Learning\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"9KwtFF_p3fLa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761035389347,"user_tz":-120,"elapsed":16297,"user":{"displayName":"Maria Chiara Ierovante","userId":"17553258909077764877"}},"outputId":"28eddbea-69c1-4e35-f0c8-a5510883ccfb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["# ---------------------------------------------- Part #2 ----------------------------------------------"],"metadata":{"id":"07NbUGEyD6KM"}},{"cell_type":"markdown","metadata":{"id":"rcnQrEe1I78n"},"source":["**1. Install useful libraries**\n"]},{"cell_type":"code","source":["import os\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm"],"metadata":{"collapsed":true,"id":"M0XB1m9Nz6TO","executionInfo":{"status":"ok","timestamp":1761035446072,"user_tz":-120,"elapsed":8,"user":{"displayName":"Maria Chiara Ierovante","userId":"17553258909077764877"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Show versioning of deep learning libraries\n","import torch, torchvision\n","print(torch.__version__, torch.cuda.is_available())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9eg9MYpmGtSQ","executionInfo":{"status":"ok","timestamp":1761035458472,"user_tz":-120,"elapsed":11260,"user":{"displayName":"Maria Chiara Ierovante","userId":"17553258909077764877"}},"outputId":"977ef9e3-6bae-43af-9baf-f7279fdeb054"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["2.8.0+cu126 True\n"]}]},{"cell_type":"code","source":["# Directory that contains all the data/script of this lab\n","current_dir = '/content/drive/MyDrive/Colab Notebooks/eim/lab3'"],"metadata":{"id":"JAEcPhMoGpaP","executionInfo":{"status":"ok","timestamp":1761035459931,"user_tz":-120,"elapsed":3,"user":{"displayName":"Maria Chiara Ierovante","userId":"17553258909077764877"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["**2. Define U-Net model**"],"metadata":{"id":"Pzjfw8fNwdU9"}},{"cell_type":"code","source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# U-Net è un'architettura a encoder-decoder con skip connections\n","\n","\"\"\"\n","Input Image (3, 256, 256)\n","    ↓\n","┌─────────────────────────────────────┐\n","│  ENCODER (Downsampling)             │\n","│  DoubleConv → MaxPool → DoubleConv  │\n","│  ┌────────────┐                     │\n","│  │SkipConnect │ (copia feature)     │\n","│  └────────────┘                     │\n","└─────────────────────────────────────┘\n","    ↓\n","┌─────────────────────────────────────┐\n","│  BOTTLENECK (livello più profondo)  │\n","└─────────────────────────────────────┘\n","    ↓\n","┌─────────────────────────────────────┐\n","│  DECODER (Upsampling)               │\n","│  Upsample → Concatena skip          │\n","│  → DoubleConv                       │\n","└─────────────────────────────────────┘\n","    ↓\n","Output Mask (1, 256, 256)\n","\"\"\"\n","\n","class DoubleConv(nn.Module):\n","  # blocco riutilizzabile che applica 2 convoluzioni consecutive\n","  # (perchè se usiamo 2 convoluzioni posso estrerre features più complesse e non lineari)\n","    \"\"\"Applies two consecutive conv-batchnorm-relu layers\"\"\"\n","    def __init__(self, in_channels, out_channels):\n","        super(DoubleConv, self).__init__()\n","        self.double_conv = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","            # nella prima concoluzione il kernel usato è 3x3 e si aggiunge 1 pixel di bordo\n","            nn.BatchNorm2d(out_channels),\n","            # le attivazioni vengono normalizzate\n","            nn.ReLU(inplace=True),\n","\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n","            # seconda attivazione\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        return self.double_conv(x)\n","\n","class UNet(nn.Module):\n","    def __init__(self,\n","                 in_channels=3,\n","                 out_channels=1,\n","                 init_filters=64, # primo layer ha 64 filtri, poi 128, 256, 512...\n","                 depth=4, # 4 livelli di downsampling (256→128→64→32→16)\n","                 bilinear=True): # usa interpolazione bilineare per upsampling\n","        super(UNet, self).__init__()\n","        self.depth = depth\n","        self.down_layers = nn.ModuleList()\n","        self.up_layers = nn.ModuleList()\n","        self.pool = nn.MaxPool2d(2)\n","\n","        # Encoder\n","        filters = init_filters\n","        for d in range(depth):\n","            conv = DoubleConv(in_channels, filters)\n","            self.down_layers.append(conv)\n","            in_channels = filters\n","            filters *= 2\n","\n","        # Bottleneck\n","        self.bottleneck = DoubleConv(in_channels, filters)\n","\n","        # Decoder\n","        for d in range(depth):\n","            filters //= 2 # Dimezza i filtri: 1024→512→256→128→64\n","            if bilinear:\n","              # raddoppio dimensione spaziale a 32x32, interpolazione spaziale e riduzione canali con conv 1x1\n","                up = nn.Sequential(\n","                    nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n","                    nn.Conv2d(filters * 2, filters, kernel_size=1)\n","                )\n","            else:\n","              # convoluzione trasporta, più parametri-> impara come fare upsampling\n","                up = nn.ConvTranspose2d(filters * 2, filters, kernel_size=2, stride=2)\n","            self.up_layers.append(nn.ModuleDict({\n","                'up': up,\n","                'conv': DoubleConv(filters * 2, filters)\n","            }))\n","\n","        # Output layer\n","        self.out_conv = nn.Conv2d(init_filters, out_channels, kernel_size=1)\n","        # riduce da 64 canali a 1 canale (maschera)\n","        # il kernel:size = 1 non cambia la dimensione spaziale\n","\n","    def forward(self, x):\n","        skip_connections = []\n","        for down in self.down_layers:\n","            x = down(x)\n","            skip_connections.append(x)\n","            x = self.pool(x)\n","\n","        x = self.bottleneck(x)\n","        # processa il livello più profondo\n","        # Input: (512, 16, 16) → Output: (1024, 16, 16)\n","\n","        for i in range(self.depth):\n","            skip = skip_connections[-(i+1)]\n","            # decoder va dal basso verso l'alto, le skip connections vanno dall'alto verso il basso\n","            # -(i+1) inverte l'ordine -> i = 0 ultimo salvato, i = 1 penultimo ...\n","            up = self.up_layers[i]['up'](x)\n","            if up.size() != skip.size():\n","                # Resize in case of odd size mismatch\n","                up = F.interpolate(up, size=skip.shape[2:])\n","            # se le dimensionoi non combaciano perfettamente, ridimensiona\n","            x = torch.cat([skip, up], dim=1)\n","            x = self.up_layers[i]['conv'](x)\n","\n","        return self.out_conv(x)\n","\n","        # quando il encoder ha più filtri riesco a catturare più features complesse\n","        # quando il decoder con meno filtri riesce a ricostruire più dettagli"],"metadata":{"id":"bBb3ikrXHJ7X","executionInfo":{"status":"ok","timestamp":1761035461841,"user_tz":-120,"elapsed":10,"user":{"displayName":"Maria Chiara Ierovante","userId":"17553258909077764877"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["Define and test a sample model with"],"metadata":{"id":"Pccnhe7OJ6mZ"}},{"cell_type":"code","source":["# Test the model\n","model = UNet(in_channels=3, out_channels=1, init_filters=32, depth=3)\n","x = torch.randn(1, 3, 256, 256)  # example input\n","out = model(x)\n","print(out.shape)  # should be [1, 1, 256, 256]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JQMk05J4HJ0m","executionInfo":{"status":"ok","timestamp":1761034544318,"user_tz":-120,"elapsed":656,"user":{"displayName":"Maria Chiara Ierovante","userId":"17553258909077764877"}},"outputId":"a773c0dc-69d3-4eb7-c47e-4d00da2970ae"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 1, 256, 256])\n"]}]},{"cell_type":"code","source":["import torch\n","from torchsummary import summary\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = UNet(in_channels=3, out_channels=1, init_filters=32, depth=2)\n","model.to(device)\n","\n","summary(model, input_size=(3, 256, 256))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AZW_nW4VHJpI","executionInfo":{"status":"ok","timestamp":1761034548916,"user_tz":-120,"elapsed":1152,"user":{"displayName":"Maria Chiara Ierovante","userId":"17553258909077764877"}},"outputId":"80a8079b-844a-40cc-884b-11d512348784"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 32, 256, 256]             896\n","       BatchNorm2d-2         [-1, 32, 256, 256]              64\n","              ReLU-3         [-1, 32, 256, 256]               0\n","            Conv2d-4         [-1, 32, 256, 256]           9,248\n","       BatchNorm2d-5         [-1, 32, 256, 256]              64\n","              ReLU-6         [-1, 32, 256, 256]               0\n","        DoubleConv-7         [-1, 32, 256, 256]               0\n","         MaxPool2d-8         [-1, 32, 128, 128]               0\n","            Conv2d-9         [-1, 64, 128, 128]          18,496\n","      BatchNorm2d-10         [-1, 64, 128, 128]             128\n","             ReLU-11         [-1, 64, 128, 128]               0\n","           Conv2d-12         [-1, 64, 128, 128]          36,928\n","      BatchNorm2d-13         [-1, 64, 128, 128]             128\n","             ReLU-14         [-1, 64, 128, 128]               0\n","       DoubleConv-15         [-1, 64, 128, 128]               0\n","        MaxPool2d-16           [-1, 64, 64, 64]               0\n","           Conv2d-17          [-1, 128, 64, 64]          73,856\n","      BatchNorm2d-18          [-1, 128, 64, 64]             256\n","             ReLU-19          [-1, 128, 64, 64]               0\n","           Conv2d-20          [-1, 128, 64, 64]         147,584\n","      BatchNorm2d-21          [-1, 128, 64, 64]             256\n","             ReLU-22          [-1, 128, 64, 64]               0\n","       DoubleConv-23          [-1, 128, 64, 64]               0\n","         Upsample-24        [-1, 128, 128, 128]               0\n","           Conv2d-25         [-1, 64, 128, 128]           8,256\n","           Conv2d-26         [-1, 64, 128, 128]          73,792\n","      BatchNorm2d-27         [-1, 64, 128, 128]             128\n","             ReLU-28         [-1, 64, 128, 128]               0\n","           Conv2d-29         [-1, 64, 128, 128]          36,928\n","      BatchNorm2d-30         [-1, 64, 128, 128]             128\n","             ReLU-31         [-1, 64, 128, 128]               0\n","       DoubleConv-32         [-1, 64, 128, 128]               0\n","         Upsample-33         [-1, 64, 256, 256]               0\n","           Conv2d-34         [-1, 32, 256, 256]           2,080\n","           Conv2d-35         [-1, 32, 256, 256]          18,464\n","      BatchNorm2d-36         [-1, 32, 256, 256]              64\n","             ReLU-37         [-1, 32, 256, 256]               0\n","           Conv2d-38         [-1, 32, 256, 256]           9,248\n","      BatchNorm2d-39         [-1, 32, 256, 256]              64\n","             ReLU-40         [-1, 32, 256, 256]               0\n","       DoubleConv-41         [-1, 32, 256, 256]               0\n","           Conv2d-42          [-1, 1, 256, 256]              33\n","================================================================\n","Total params: 437,089\n","Trainable params: 437,089\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.75\n","Forward/backward pass size (MB): 442.50\n","Params size (MB): 1.67\n","Estimated Total Size (MB): 444.92\n","----------------------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["**3. Define hyperparameters for network training**"],"metadata":{"id":"KuG5CumRw8Ao"}},{"cell_type":"code","source":["# Network architecture\n","input_size = (256,256)\n","in_channels = 3    # 1 for grayscale, 3 for RGB images\n","out_channels = 1   # 1 for binary segmentation, N for multiclass segmentation\n","init_filters = 32\n","depth = 3\n","\n","# Training hyperparameters\n","criterion = nn.BCEWithLogitsLoss()  # loss function\n","n_epochs = 10 # Start point. Aumenta a 20-30 per risultati migliori\n","batch_size = 4 # between 2 and 8\n","learning_rate = 1e-3\n","checkpoint_freq = 1  # save every checkpoint\n","checkpoint_dir = os.path.join(current_dir,'checkpoints')  # path where checkpoints will be saved\n","\n","if not os.path.exists(checkpoint_dir):\n","    os.makedirs(checkpoint_dir)\n","\n","# Paths for dataset\n","base_dir = os.path.join(current_dir,'HAM1000')\n","\n","train_img_dir = os.path.join(base_dir, 'train', 'image')\n","train_mask_dir = os.path.join(base_dir, 'train', 'manual')\n","\n","val_img_dir = os.path.join(base_dir, 'val', 'image')\n","val_mask_dir = os.path.join(base_dir, 'val', 'manual')"],"metadata":{"id":"LLFQP1PnKeNq","executionInfo":{"status":"ok","timestamp":1761034554340,"user_tz":-120,"elapsed":2306,"user":{"displayName":"Maria Chiara Ierovante","userId":"17553258909077764877"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Save config file with the user-defined parameters of the current run\n","import json\n","\n","params = {\n","    \"input_size\": input_size,\n","    \"in_channels\": in_channels,\n","    \"out_channels\": out_channels,\n","    \"init_filters\": init_filters,\n","    \"depth\": depth,\n","    \"n_epochs\": n_epochs,\n","    \"batch_size\": batch_size,\n","    \"learning_rate\": learning_rate,\n","    \"checkpoint_freq\": checkpoint_freq,\n","}\n","\n","params_path = os.path.join(checkpoint_dir, 'training_params.json')\n","with open(params_path, 'w') as f:\n","    json.dump(params, f, indent=4)\n","print(f\"Training parameters saved to {params_path}\")"],"metadata":{"id":"NRLeMQKqRIqA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761034558462,"user_tz":-120,"elapsed":739,"user":{"displayName":"Maria Chiara Ierovante","userId":"17553258909077764877"}},"outputId":"cbf9baf5-9951-4141-b041-8157d190a894"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Training parameters saved to /content/drive/MyDrive/Colab Notebooks/eim/lab3/checkpoints/training_params.json\n"]}]},{"cell_type":"markdown","source":["Dataloader definition"],"metadata":{"id":"-vs7WG9kLiTC"}},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","\n","# pipeline per caricamento dati per il training della U-Net\n","class DermaDataset(Dataset):\n","    def __init__(self, image_dir, mask_dir, transform=None):\n","      #salva i percorsi delle cartelle e le trasformazioni\n","        self.image_dir = image_dir\n","        self.mask_dir = mask_dir\n","        self.transform = transform\n","        self.image_list = sorted(os.listdir(image_dir))\n","        self.mask_list = sorted(os.listdir(mask_dir))\n","        assert len(self.image_list) == len(self.mask_list), \"Number of images and masks do not match\"\n","\n","    def __len__(self):\n","        return len(self.image_list)\n","        # ritorna quanti campioni ci sono nel dataset\n","\n","    def __getitem__(self, idx):\n","      # viene chiamata ogni volta che il DataLocker prende un dato\n","        img_path = os.path.join(self.image_dir, self.image_list[idx])\n","        mask_path = os.path.join(self.mask_dir, self.mask_list[idx])\n","\n","        image = Image.open(img_path).convert('RGB')\n","        mask = Image.open(mask_path).convert('L')  # grayscale mask\n","\n","        if self.transform:\n","            image = self.transform(image)\n","            mask = self.transform(mask)\n","\n","        # Convert mask to binary float tensor (0 or 1)\n","        mask = (mask > 0).float()\n","\n","        return image, mask\n","\n","\n","# Transformations applied to both images and masks\n","transform = transforms.Compose([\n","    transforms.Resize(input_size),\n","    transforms.ToTensor(),\n","])\n","\n","# Initialize datasets and corresponding dataloaders\n","train_dataset = DermaDataset(train_img_dir, train_mask_dir, transform=transform)\n","val_dataset = DermaDataset(val_img_dir, val_mask_dir, transform=transform)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n","val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"],"metadata":{"id":"3TlDE814KeI3","executionInfo":{"status":"ok","timestamp":1761034565002,"user_tz":-120,"elapsed":3925,"user":{"displayName":"Maria Chiara Ierovante","userId":"17553258909077764877"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["**4. Actual training**"],"metadata":{"id":"ohWPnHFMzHCF"}},{"cell_type":"code","source":["import torch.optim as optim\n","\n","# Model initialization\n","model = UNet(in_channels, out_channels, init_filters, depth).to(device)\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)"],"metadata":{"id":"U0a2M3VrKkft","executionInfo":{"status":"ok","timestamp":1761034567154,"user_tz":-120,"elapsed":34,"user":{"displayName":"Maria Chiara Ierovante","userId":"17553258909077764877"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import jaccard_score\n","\n","for epoch in range(n_epochs):\n","    model.train()\n","    train_loss = 0.0  # loss should be initialized to 0\n","\n","    for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs} - Training\"):\n","        images = images.to(device)\n","        masks = masks.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)  # logits\n","\n","        loss = criterion(outputs, masks)\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item() * images.size(0)\n","\n","    train_loss /= len(train_loader.dataset)\n","    print(f\"Epoch {epoch+1} Train Loss: {train_loss:.4f}\")\n","\n","    # Validation\n","    model.eval()\n","    val_loss = 0.0\n","    all_preds = []\n","    all_masks = []\n","\n","    with torch.no_grad():\n","        for images, masks in val_loader:\n","            images = images.to(device)\n","            masks = masks.to(device)\n","            outputs = model(images)\n","\n","            loss = criterion(outputs, masks)\n","            val_loss += loss.item() * images.size(0)\n","\n","            if out_channels == 1:\n","                # Binary segmentation: sigmoid + threshold\n","                probs = torch.sigmoid(outputs)\n","                preds = (probs > 0.5).long()\n","                true = masks.long()\n","            else:\n","                # Multi-class segmentation: argmax\n","                preds = torch.argmax(outputs, dim=1)\n","                true = masks.long()\n","\n","            all_preds.append(preds.cpu().numpy().flatten())\n","            all_masks.append(true.cpu().numpy().flatten())\n","\n","    val_loss /= len(val_loader.dataset)\n","\n","    # mIoU (Jaccard) metric\n","    y_true = np.concatenate(all_masks)\n","    y_pred = np.concatenate(all_preds)\n","\n","    iou = jaccard_score(y_true, y_pred, average='macro')\n","\n","    print(f\"Epoch {epoch+1} Val Loss: {val_loss:.4f} - Val IoU: {iou:.4f}\")\n","\n","    # Save checkpoint every [checkpoint_freq] epochs\n","    if (epoch + 1) % checkpoint_freq == 0:\n","        checkpoint_path = os.path.join(checkpoint_dir, f\"model_epoch_{epoch+1}.pt\")\n","        torch.save(model.state_dict(), checkpoint_path)\n","        print(f\"Checkpoint saved at {checkpoint_path}\")\n","\n","print(\"Training complete!\")\n"],"metadata":{"id":"D8C12-B6KkYZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761034678210,"user_tz":-120,"elapsed":108537,"user":{"displayName":"Maria Chiara Ierovante","userId":"17553258909077764877"}},"outputId":"84a4e2f7-1468-42af-d0d7-50f110957cc5"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1/10 - Training: 100%|██████████| 52/52 [00:38<00:00,  1.35it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 1 Train Loss: 0.4285\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1 Val Loss: 0.5409 - Val IoU: 0.6115\n","Checkpoint saved at /content/drive/MyDrive/Colab Notebooks/eim/lab3/checkpoints/model_epoch_1.pt\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/10 - Training: 100%|██████████| 52/52 [00:04<00:00, 11.65it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 2 Train Loss: 0.3625\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2 Val Loss: 0.3996 - Val IoU: 0.6514\n","Checkpoint saved at /content/drive/MyDrive/Colab Notebooks/eim/lab3/checkpoints/model_epoch_2.pt\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/10 - Training: 100%|██████████| 52/52 [00:04<00:00, 12.26it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 3 Train Loss: 0.3583\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3 Val Loss: 0.4390 - Val IoU: 0.6268\n","Checkpoint saved at /content/drive/MyDrive/Colab Notebooks/eim/lab3/checkpoints/model_epoch_3.pt\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/10 - Training: 100%|██████████| 52/52 [00:04<00:00, 12.15it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 4 Train Loss: 0.3128\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4 Val Loss: 0.3619 - Val IoU: 0.6272\n","Checkpoint saved at /content/drive/MyDrive/Colab Notebooks/eim/lab3/checkpoints/model_epoch_4.pt\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/10 - Training: 100%|██████████| 52/52 [00:04<00:00, 11.82it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 5 Train Loss: 0.3210\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5 Val Loss: 0.4203 - Val IoU: 0.6551\n","Checkpoint saved at /content/drive/MyDrive/Colab Notebooks/eim/lab3/checkpoints/model_epoch_5.pt\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/10 - Training: 100%|██████████| 52/52 [00:04<00:00, 12.12it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 6 Train Loss: 0.3204\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6 Val Loss: 0.4137 - Val IoU: 0.5974\n","Checkpoint saved at /content/drive/MyDrive/Colab Notebooks/eim/lab3/checkpoints/model_epoch_6.pt\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/10 - Training: 100%|██████████| 52/52 [00:04<00:00, 11.39it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 7 Train Loss: 0.2956\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7 Val Loss: 0.3428 - Val IoU: 0.6710\n","Checkpoint saved at /content/drive/MyDrive/Colab Notebooks/eim/lab3/checkpoints/model_epoch_7.pt\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/10 - Training: 100%|██████████| 52/52 [00:04<00:00, 12.13it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 8 Train Loss: 0.2902\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8 Val Loss: 0.3843 - Val IoU: 0.6889\n","Checkpoint saved at /content/drive/MyDrive/Colab Notebooks/eim/lab3/checkpoints/model_epoch_8.pt\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9/10 - Training: 100%|██████████| 52/52 [00:04<00:00, 11.67it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 9 Train Loss: 0.3042\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9 Val Loss: 0.3344 - Val IoU: 0.6924\n","Checkpoint saved at /content/drive/MyDrive/Colab Notebooks/eim/lab3/checkpoints/model_epoch_9.pt\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10/10 - Training: 100%|██████████| 52/52 [00:04<00:00, 12.05it/s]"]},{"output_type":"stream","name":"stdout","text":["Epoch 10 Train Loss: 0.2835\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10 Val Loss: 0.3618 - Val IoU: 0.6828\n","Checkpoint saved at /content/drive/MyDrive/Colab Notebooks/eim/lab3/checkpoints/model_epoch_10.pt\n","Training complete!\n"]}]},{"cell_type":"markdown","source":["Extra (do not run until the end of the part #3)"],"metadata":{"id":"WJrAYN_XdO7V"}},{"cell_type":"code","source":["class DiceLoss(nn.Module):\n","    def __init__(self, smooth=1e-6):\n","        super(DiceLoss, self).__init__()\n","        self.smooth = smooth\n","\n","    def forward(self, inputs, targets):\n","        # inputs: raw logits -> apply sigmoid\n","        inputs = torch.sigmoid(inputs)\n","\n","        # Flatten\n","        inputs = inputs.view(-1)\n","        targets = targets.view(-1)\n","\n","        intersection = (inputs * targets).sum()\n","        dice = (2. * intersection + self.smooth) / (\n","            inputs.sum() + targets.sum() + self.smooth\n","        )\n","\n","        return 1 - dice  # Dice Loss"],"metadata":{"id":"qB5kQONvdMtY","executionInfo":{"status":"ok","timestamp":1761035466852,"user_tz":-120,"elapsed":4,"user":{"displayName":"Maria Chiara Ierovante","userId":"17553258909077764877"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["criterion = DiceLoss()"],"metadata":{"id":"8dEdrOt9c1zj","executionInfo":{"status":"ok","timestamp":1761035473389,"user_tz":-120,"elapsed":4,"user":{"displayName":"Maria Chiara Ierovante","userId":"17553258909077764877"}}},"execution_count":9,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1BdRKq8bj4fH9SCOEDqk-3cuh2Qh5h17X","timestamp":1634731372127},{"file_id":"15u0PXxVE3F5Qa5B3eKEJ2Rkf4Z1i0Q8V","timestamp":1634720935124}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}